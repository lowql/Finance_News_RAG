#  大語言模型落地

一般在專業領域，要實際落地使用LLM需要考量以下幾點:
1. 回答專業領域問題的精確性 
	1. 資料來源是否為第一手資料
	2. 專業領域的特定名詞使用方式
2. 模型對多能處理的`上下文長度`
3. (本地運行)
	1. 模型大小 : 本地的運算資源，最多可以支持多大的`模型參數量`
	2. 模型的可靠性: `量化`後的模型，`抗雜訊能力`下降
4. (雲端運行)
	1. 使用 `Token` 的費率問題
## Fine-tunning 微調訓練 LLM 

訓練完後語言模型會思考過後給出相對應的解答。
- 優點： 語言模型會真的知道這些知識與資料, 回答上也較為快速。
- 缺點： 
	- 有的語言模型不提供 Fine-tunning API
	- Fine-tunning 過程`收費高昂` 
	- `本地資料量不足`，成效不一定好, 因為我們給的資料對於它原本的參數來說只是小蝦米
	- 上傳資料的`隱私性`問題。

## RAG (資料檢索生成)
給定資料文件, 將`文件轉成空間項量化的資料`後, 根據目前問題找尋最相關的向量資料, 檢索後將資料餵給模型理解, 最後給出它的答案。
- 優點： 不用花那麼多時間做前置訓練處理、如果資料給對的話答案較為精準。不用上傳全部文件, 也比較安全。
- 缺點： 須先 embeding 文件, 因為是使用類似 prompt 加入訊息的方式給模型, 訊息內容較多, 所以每次回答上都會耗費更多時間。

# RAG 遇到的困境

1. 當模型從70B量化到8B時，對處理複雜訊息的和抗造生的能力都會顯著下降，直接影響實際回覆的準確性。
2. 檢索策略做的不好，很有可能會超過模型本身的上下文長度限制，並降低生成內容的相關性與準確性。

### 提升精確度的策略

#### 使用 Neo4j 建立知識圖譜

##### 知識圖譜本身的可觀測性，維護資料的可靠性

##### 使用 vector index 進一步提升，資訊檢索的效率與精確度
[[neo4j index X llama-index]]

通過使用 vector index 可以在結構化與非結構化數據上進行高效率的語意檢索，通過HNSW算法，Neo4j可以快速找到相似的節點。
### 挑戰上下文長度的限制

#### 使用本地 LLM  對訊息預先做總結


